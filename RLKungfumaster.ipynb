{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tjqGhmW0S72"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "gamma = 0.99\n",
        "training_episodes = 2000\n",
        "some_threshold = 5000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu, linear\n",
        "from tensorflow.keras.losses import mean_squared_error\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import wandb\n",
        "class DQN:\n",
        "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
        "        self.env = env\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.replay_memory_buffer = deque(maxlen=500000)\n",
        "        self.batch_size = 64\n",
        "        self.epsilon_min = 0.01\n",
        "        self.num_action_space = env.action_space.n\n",
        "        self.num_observation_space = np.prod(env.observation_space.shape)\n",
        "        self.model = self.initialize_model()\n",
        "\n",
        "    def initialize_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(512, input_dim=self.num_observation_space, activation=relu))\n",
        "        model.add(Dense(256, activation=relu))\n",
        "        model.add(Dense(self.num_action_space, activation=linear))\n",
        "        model.compile(loss=mean_squared_error, optimizer=Adam(learning_rate=self.lr))\n",
        "        return model\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.randrange(self.num_action_space)\n",
        "        predicted_actions = self.model.predict(state)\n",
        "        return np.argmax(predicted_actions[0])\n",
        "\n",
        "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def learn_and_update_weights_by_reply(self):\n",
        "        if len(self.replay_memory_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        random_sample = self.get_random_sample_from_replay_mem()\n",
        "        states, actions, rewards, next_states, done_list = self.get_attribues_from_sample(random_sample)\n",
        "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
        "        target_vec = self.model.predict_on_batch(states)\n",
        "        indexes = np.array([i for i in range(self.batch_size)])\n",
        "        target_vec[[indexes], [actions]] = targets\n",
        "\n",
        "        history = self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
        "        loss = history.history['loss'][0]\n",
        "        wandb.log({'Loss': loss})\n",
        "\n",
        "    def get_attribues_from_sample(self, random_sample):\n",
        "        states = np.array([i[0] for i in random_sample])\n",
        "        actions = np.array([i[1] for i in random_sample])\n",
        "        rewards = np.array([i[2] for i in random_sample])\n",
        "        next_states = np.array([i[3] for i in random_sample])\n",
        "        done_list = np.array([i[4] for i in random_sample])\n",
        "        states = np.squeeze(states)\n",
        "        next_states = np.squeeze(next_states)\n",
        "        return states, actions, rewards, next_states, done_list\n",
        "\n",
        "    def get_random_sample_from_replay_mem(self):\n",
        "        return random.sample(self.replay_memory_buffer, self.batch_size)\n",
        "\n",
        "    def save_training_progress(self, rewards_list, episode, epsilon):\n",
        "        with open('training_progress.txt', 'a') as file:\n",
        "            file.write(f'Episode: {episode}, Average Reward: {sum(rewards_list)/len(rewards_list)}, Epsilon: {epsilon}\\n')\n",
        "\n",
        "    def train(self, num_episodes, can_stop=True):\n",
        "        rewards_list = []\n",
        "        for episode in range(num_episodes):\n",
        "            initial_state = self.env.reset()\n",
        "            state = initial_state[0] if isinstance(initial_state, tuple) else initial_state\n",
        "            state_flattened = state.flatten()\n",
        "            state = np.reshape(state_flattened, [1, self.num_observation_space])\n",
        "\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            step = 0\n",
        "            while not done:\n",
        "                action = self.get_action(state)\n",
        "                step_result = self.env.step(action)\n",
        "\n",
        "                next_state = step_result[0]\n",
        "                reward = step_result[1]\n",
        "                done = step_result[2]\n",
        "\n",
        "                next_state_flattened = next_state.flatten()\n",
        "                next_state = np.reshape(next_state_flattened, [1, self.num_observation_space])\n",
        "\n",
        "                self.add_to_replay_memory(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "                self.learn_and_update_weights_by_reply()\n",
        "\n",
        "                step += 1\n",
        "                if step % 100 == 0:\n",
        "                    wandb.log({'Episode': episode, 'Step': step, 'Total Reward (Step)': total_reward})\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            rewards_list.append(total_reward)\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "            wandb.log({'Episode': episode, 'Total Reward (Episode)': total_reward, 'Epsilon': self.epsilon})\n",
        "        # Erken durma koşulu\n",
        "            if can_stop and np.mean(rewards_list[-100:]) > some_threshold:\n",
        "                print(f\"Erken durma koşulu {episode} bölümünde karşılandı.\")\n",
        "                break\n",
        "\n",
        "            if episode % 100 == 0 or episode == num_episodes - 1:\n",
        "                self.save_training_progress(rewards_list, episode, self.epsilon)\n",
        "\n",
        "        return rewards_list"
      ],
      "metadata": {
        "id": "v18HHKML0f8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def plot_df(df, chart_name, title, x_axis_label, y_axis_label):\n",
        "    \"\"\"\n",
        "    Verilen DataFrame üzerinden çizgi grafiği çizer ve kaydeder.\n",
        "\n",
        "    :param df: Çizim için kullanılacak pandas DataFrame.\n",
        "    :param chart_name: Grafiğin kaydedileceği dosya adı.\n",
        "    :param title: Grafiğin başlığı.\n",
        "    :param x_axis_label: X ekseni için etiket.\n",
        "    :param y_axis_label: Y ekseni için etiket.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.plot(df)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_axis_label)\n",
        "    plt.ylabel(y_axis_label)\n",
        "    plt.savefig(chart_name)\n",
        "    plt.close()\n",
        "\n",
        "def save_model(model, filename):\n",
        "    \"\"\"\n",
        "    Verilen modeli belirtilen dosya adıyla kaydeder.\n",
        "\n",
        "    :param model: Kaydedilecek model.\n",
        "    :param filename: Modelin kaydedileceği dosya adı.\n",
        "    \"\"\"\n",
        "    model.save(filename)\n",
        "\n",
        "def load_trained_model(filename):\n",
        "    \"\"\"\n",
        "    Belirtilen dosya adından eğitilmiş bir model yükler.\n",
        "\n",
        "    :param filename: Yüklenecek modelin dosya adı.\n",
        "    :return: Yüklenen model.\n",
        "    \"\"\"\n",
        "    return load_model(filename)\n",
        "\n",
        "def save_to_pickle(data, filename):\n",
        "    \"\"\"\n",
        "    Verilen veriyi pickle formatında kaydeder.\n",
        "\n",
        "    :param data: Kaydedilecek veri.\n",
        "    :param filename: Dosya adı.\n",
        "    \"\"\"\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "def load_from_pickle(filename):\n",
        "    \"\"\"\n",
        "    Pickle formatında kaydedilmiş veriyi yükler.\n",
        "\n",
        "    :param filename: Yüklenmek istenen dosyanın adı.\n",
        "    :return: Yüklenen veri.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as file:\n",
        "        return pickle.load(file)\n"
      ],
      "metadata": {
        "id": "EiSl6qWt04dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "import wandb\n",
        "from dqn_agent import DQN\n",
        "from utils import plot_df, save_model\n",
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    # WandB yapılandırması\n",
        "    wandb.init(project='RKkungfumaster', entity='fth123bng')\n",
        "\n",
        "    # Gym ortamını başlat\n",
        "    env = gym.make('ALE/KungFuMaster-v5', render_mode=\"rgb_array\")\n",
        "    env.action_space.seed(42)\n",
        "    np.random.seed(21)\n",
        "\n",
        "    # DQN modelini başlat\n",
        "    model = DQN(env, lr, gamma, epsilon, epsilon_decay)\n",
        "\n",
        "    print(\"Starting training for DQN model...\")\n",
        "    training_rewards = model.train(training_episodes)\n",
        "\n",
        "    # Modeli kaydet ve WandB'a yükle\n",
        "    save_dir = \"saved_models/\"\n",
        "    model_path = save_dir + \"trained_model.h5\"\n",
        "    save_model(model.model, model_path)\n",
        "    wandb.save(model_path)\n",
        "\n",
        "    # Eğitim ödüllerini kaydet ve görselleştir\n",
        "    pickle.dump(training_rewards, open(save_dir + \"train_rewards_list.p\", \"wb\"))\n",
        "    reward_df = pd.DataFrame(training_rewards)\n",
        "    plot_df(reward_df, save_dir + \"training_rewards.png\", \"Training Rewards per Episode\", \"Episode\", \"Reward\")\n",
        "    wandb.log({\"Training Rewards\": wandb.Image(save_dir + \"training_rewards.png\")})\n",
        "\n",
        "    print(\"Training Completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "pdrQ_wPN0-S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test.py\n",
        "from utils import load_trained_model\n",
        "from utils import plot_df, load_trained_model\n",
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "def test_already_trained_model(trained_model, env, num_episodes=100):\n",
        "    test_rewards = []\n",
        "    for episode in range(num_episodes):\n",
        "        initial_state = env.reset()\n",
        "        state = initial_state[0] if isinstance(initial_state, tuple) else initial_state\n",
        "        state_flattened = state.flatten()\n",
        "        state = np.reshape(state_flattened, [1, np.prod(env.observation_space.shape)])\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.argmax(trained_model.predict(state)[0])\n",
        "            step_result = env.step(action)\n",
        "            next_state = step_result[0]\n",
        "            reward = step_result[1]\n",
        "            done = step_result[2]\n",
        "\n",
        "            next_state_flattened = next_state.flatten()\n",
        "            state = np.reshape(next_state_flattened, [1, np.prod(env.observation_space.shape)])\n",
        "\n",
        "            total_reward += reward\n",
        "        test_rewards.append(total_reward)\n",
        "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "        wandb.log({'Test Episode': episode, 'Total Reward': total_reward})\n",
        "    return test_rewards\n",
        "\n",
        "def test_model():\n",
        "    # WandB yapılandırması\n",
        "    wandb.init(project='RKkungfumaster', entity='fth123bng', job_type=\"testing\")\n",
        "\n",
        "    # Gym ortamını başlat\n",
        "    env = gym.make('ALE/KungFuMaster-v5', render_mode=\"rgb_array\")\n",
        "\n",
        "    # Eğitilmiş modeli yükle\n",
        "    save_dir = \"saved_models/\"\n",
        "    model_path = save_dir + \"trained_model.h5\"\n",
        "    trained_model = load_trained_model(model_path)\n",
        "\n",
        "    # Modeli test et\n",
        "    test_rewards = test_already_trained_model(trained_model, env)\n",
        "    pickle.dump(test_rewards, open(save_dir + \"test_rewards.p\", \"wb\"))\n",
        "    test_rewards_df = pd.DataFrame(test_rewards)\n",
        "    plot_df(test_rewards_df, save_dir + \"testing_rewards.png\", \"Testing Rewards per Episode\", \"Episode\", \"Reward\")\n",
        "    wandb.log({\"Testing Rewards\": wandb.Image(save_dir + \"testing_rewards.png\")})\n",
        "\n",
        "    print(\"Testing Completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()"
      ],
      "metadata": {
        "id": "LBM_BE-T1HrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}