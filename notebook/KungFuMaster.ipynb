{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YOU SHOULD CHOOSE TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IjmIJh81s1E",
        "outputId": "84c22e17-a0b6-4ced-a57f-feaefb24cee6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy gym pandas keras stable-baselines3 atari_py gym[accept-rom-license] wandb atari-py dopamine-rl shimmy torch gymnasium gymnasium[ale] gymnasium[atari]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# TPU detektörü\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU bulunursa\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "# TPU'yu kullanmak için güncellenmiş TensorFlow stratejisini ayarla\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)  # Güncellenmiş strateji kullanımı\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # TPU yoksa varsayılan stratejiyi kullan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tjqGhmW0S72"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "gamma = 0.99\n",
        "training_episodes = 1\n",
        "some_threshold = 10000\n",
        "num_envs= 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v18HHKML0f8x"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu, linear\n",
        "from tensorflow.keras.losses import mean_squared_error\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import wandb\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
        "        self.env = env\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.replay_memory_buffer = deque(maxlen=15000)\n",
        "        self.batch_size = 64\n",
        "        self.epsilon_min = 0.01\n",
        "        self.num_envs = env.num_envs\n",
        "        self.num_action_space = env.action_space.n\n",
        "        self.num_observation_space = np.prod(env.observation_space.shape)\n",
        "        self.model = self.initialize_model()\n",
        "\n",
        "    def initialize_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(512, input_dim=self.num_observation_space, activation=relu))\n",
        "        model.add(Dense(256, activation=relu))\n",
        "        model.add(Dense(self.num_action_space, activation=linear))\n",
        "        model.compile(loss=mean_squared_error, optimizer=Adam(learning_rate=self.lr))\n",
        "        return model\n",
        "\n",
        "    def get_action(self, states):\n",
        "        actions = []\n",
        "        for state in states:\n",
        "            if np.random.rand() < self.epsilon:\n",
        "                actions.append(random.randrange(self.num_action_space))\n",
        "            else:\n",
        "                predicted_actions = self.model.predict(state)\n",
        "                actions.append(np.argmax(predicted_actions[0]))\n",
        "        return actions\n",
        "\n",
        "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def learn_and_update_weights_by_reply(self):\n",
        "        if len(self.replay_memory_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        random_sample = self.get_random_sample_from_replay_mem()\n",
        "        states, actions, rewards, next_states, done_list = self.get_attribues_from_sample(random_sample)\n",
        "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
        "        target_vec = self.model.predict_on_batch(states)\n",
        "        indexes = np.array([i for i in range(self.batch_size)])\n",
        "        target_vec[[indexes], [actions]] = targets\n",
        "\n",
        "        history = self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
        "        loss = history.history['loss'][0]\n",
        "        wandb.log({'Loss': loss})\n",
        "\n",
        "    def get_attribues_from_sample(self, random_sample):\n",
        "        states = np.array([i[0] for i in random_sample])\n",
        "        actions = np.array([i[1] for i in random_sample])\n",
        "        rewards = np.array([i[2] for i in random_sample])\n",
        "        next_states = np.array([i[3] for i in random_sample])\n",
        "        done_list = np.array([i[4] for i in random_sample])\n",
        "        states = np.squeeze(states)\n",
        "        next_states = np.squeeze(next_states)\n",
        "        return states, actions, rewards, next_states, done_list\n",
        "\n",
        "    def get_random_sample_from_replay_mem(self):\n",
        "        return random.sample(self.replay_memory_buffer, self.batch_size)\n",
        "\n",
        "    def save_training_progress(self, rewards_list, episode, epsilon):\n",
        "        with open('training_progress.txt', 'a') as file:\n",
        "            file.write(f'Episode: {episode}, Average Reward: {sum(rewards_list)/len(rewards_list)}, Epsilon: {epsilon}\\n')\n",
        "\n",
        "    def train(self, num_episodes, can_stop=True):\n",
        "        rewards_list = [[] for _ in range(self.num_envs)]\n",
        "        for episode in range(num_episodes):\n",
        "            states = self.env.reset()  # Tüm ortamları sıfırla\n",
        "            states = np.array([np.reshape(state.flatten(), [1, self.num_observation_space]) for state in states])\n",
        "\n",
        "            total_rewards = [0 for _ in range(self.num_envs)]\n",
        "            dones = [False for _ in range(self.num_envs)]\n",
        "\n",
        "            step = 0\n",
        "            while not all(dones):\n",
        "                actions = self.get_action(states)  # Her ortam için ayrı bir eylem seç\n",
        "                next_states, rewards, dones, _ = self.env.step(actions)\n",
        "\n",
        "                # Her ortam için verileri işle\n",
        "                for i in range(self.num_envs):\n",
        "                    state = states[i]\n",
        "                    action = actions[i]\n",
        "                    reward = rewards[i]\n",
        "                    next_state = next_states[i]\n",
        "                    done = dones[i]\n",
        "\n",
        "                    next_state = np.reshape(next_state.flatten(), [1, self.num_observation_space])\n",
        "\n",
        "                    self.add_to_replay_memory(state, action, reward, next_state, done)\n",
        "                    total_rewards[i] += reward\n",
        "\n",
        "                    if step % 100 == 0:\n",
        "                        wandb.log({'Episode': episode, 'Env': i, 'Step': step, 'Total Reward (Step)': total_rewards[i]})\n",
        "\n",
        "                states = np.array([np.reshape(state.flatten(), [1, self.num_observation_space]) for state in next_states])\n",
        "                self.learn_and_update_weights_by_reply()\n",
        "                step += 1\n",
        "\n",
        "                if step % 100 == 0:\n",
        "                    for i in range(self.num_envs):\n",
        "                        wandb.log({'Episode': episode, 'Env': i, 'Total Reward (Episode)': total_rewards[i]})\n",
        "\n",
        "            for i in range(self.num_envs):\n",
        "                rewards_list[i].append(total_rewards[i])\n",
        "                wandb.log({'Episode': episode, 'Env': i, 'Total Reward (Episode)': total_rewards[i]})\n",
        "\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)  # Epsilon güncelleme\n",
        "\n",
        "            # Erken durma koşulu\n",
        "            if can_stop and all(np.mean(rewards[-100:]) > some_threshold for rewards in rewards_list):\n",
        "                print(f\"Erken durma koşulu {episode} bölümünde karşılandı.\")\n",
        "                break\n",
        "\n",
        "            if episode % 100 == 0 or episode == num_episodes - 1:\n",
        "                self.save_training_progress(rewards_list, episode, self.epsilon)\n",
        "\n",
        "        return rewards_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiSl6qWt04dq"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def plot_df(df, chart_name, title, x_axis_label, y_axis_label):\n",
        "    \"\"\"\n",
        "    Verilen DataFrame üzerinden çizgi grafiği çizer ve kaydeder.\n",
        "\n",
        "    :param df: Çizim için kullanılacak pandas DataFrame.\n",
        "    :param chart_name: Grafiğin kaydedileceği dosya adı.\n",
        "    :param title: Grafiğin başlığı.\n",
        "    :param x_axis_label: X ekseni için etiket.\n",
        "    :param y_axis_label: Y ekseni için etiket.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.plot(df)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_axis_label)\n",
        "    plt.ylabel(y_axis_label)\n",
        "    plt.savefig(chart_name)\n",
        "    plt.close()\n",
        "\n",
        "def save_model(model, filename):\n",
        "    \"\"\"\n",
        "    Verilen modeli belirtilen dosya adıyla kaydeder.\n",
        "\n",
        "    :param model: Kaydedilecek model.\n",
        "    :param filename: Modelin kaydedileceği dosya adı.\n",
        "    \"\"\"\n",
        "    model.save(filename)\n",
        "\n",
        "def load_trained_model(filename):\n",
        "    \"\"\"\n",
        "    Belirtilen dosya adından eğitilmiş bir model yükler.\n",
        "\n",
        "    :param filename: Yüklenecek modelin dosya adı.\n",
        "    :return: Yüklenen model.\n",
        "    \"\"\"\n",
        "    return load_model(filename)\n",
        "\n",
        "def save_to_pickle(data, filename):\n",
        "    \"\"\"\n",
        "    Verilen veriyi pickle formatında kaydeder.\n",
        "\n",
        "    :param data: Kaydedilecek veri.\n",
        "    :param filename: Dosya adı.\n",
        "    \"\"\"\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "def load_from_pickle(filename):\n",
        "    \"\"\"\n",
        "    Pickle formatında kaydedilmiş veriyi yükler.\n",
        "\n",
        "    :param filename: Yüklenmek istenen dosyanın adı.\n",
        "    :return: Yüklenen veri.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as file:\n",
        "        return pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "pdrQ_wPN0-S-",
        "outputId": "85b99bf2-b3ea-4b82-8cde-5b6578e1452a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# train.py\n",
        "import wandb\n",
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "def make_env():\n",
        "    def _init():\n",
        "        env = gym.make('ALE/KungFuMaster-v5', render_mode=\"rgb_array\")\n",
        "        env.action_space.seed(42)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "def main():\n",
        "    wandb.init(project='RLkungfumaster', entity='fth123bng')\n",
        "\n",
        "    # Gym ortamını başlat\n",
        "    envs = [make_env() for _ in range(num_envs)]\n",
        "    vec_env = DummyVecEnv(envs)\n",
        "\n",
        "    # DQN modelini başlat\n",
        "    model = DQN(vec_env, lr, gamma, epsilon, epsilon_decay)\n",
        "\n",
        "    print(\"Starting training for DQN model...\")\n",
        "    training_rewards = model.train(training_episodes)\n",
        "\n",
        "    save_dir = \"saved_models/\"\n",
        "    model_path = save_dir + \"trained_model.h5\"\n",
        "    save_model(model.model, model_path)\n",
        "    wandb.save(model_path)\n",
        "\n",
        "    # Eğitim ödüllerini kaydet ve görselleştir\n",
        "    pickle.dump(training_rewards, open(save_dir + \"train_rewards_list.p\", \"wb\"))\n",
        "    reward_df = pd.DataFrame(training_rewards)\n",
        "    plot_df(reward_df, save_dir + \"training_rewards.png\", \"Training Rewards per Episode\", \"Episode\", \"Reward\")\n",
        "    wandb.log({\"Training Rewards\": wandb.Image(save_dir + \"training_rewards.png\")})\n",
        "\n",
        "    print(\"Training Completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBM_BE-T1HrA"
      },
      "outputs": [],
      "source": [
        "# test.py\n",
        "from utils import load_trained_model\n",
        "from utils import plot_df, load_trained_model\n",
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "def test_already_trained_model(trained_model, env, num_episodes=100):\n",
        "    test_rewards = []\n",
        "    for episode in range(num_episodes):\n",
        "        initial_state = env.reset()\n",
        "        state = initial_state[0] if isinstance(initial_state, tuple) else initial_state\n",
        "        state_flattened = state.flatten()\n",
        "        state = np.reshape(state_flattened, [1, np.prod(env.observation_space.shape)])\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.argmax(trained_model.predict(state)[0])\n",
        "            step_result = env.step(action)\n",
        "            next_state = step_result[0]\n",
        "            reward = step_result[1]\n",
        "            done = step_result[2]\n",
        "\n",
        "            next_state_flattened = next_state.flatten()\n",
        "            state = np.reshape(next_state_flattened, [1, np.prod(env.observation_space.shape)])\n",
        "\n",
        "            total_reward += reward\n",
        "        test_rewards.append(total_reward)\n",
        "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "        wandb.log({'Test Episode': episode, 'Total Reward': total_reward})\n",
        "    return test_rewards\n",
        "\n",
        "def test_model():\n",
        "    # WandB yapılandırması\n",
        "    wandb.init(project='RKkungfumaster', entity='fth123bng', job_type=\"testing\")\n",
        "\n",
        "    # Gym ortamını başlat\n",
        "    env = gym.make('ALE/KungFuMaster-v5', render_mode=\"rgb_array\")\n",
        "\n",
        "    # Eğitilmiş modeli yükle\n",
        "    save_dir = \"saved_models/\"\n",
        "    model_path = save_dir + \"trained_model.h5\"\n",
        "    trained_model = load_trained_model(model_path)\n",
        "\n",
        "    # Modeli test et\n",
        "    test_rewards = test_already_trained_model(trained_model, env)\n",
        "    pickle.dump(test_rewards, open(save_dir + \"test_rewards.p\", \"wb\"))\n",
        "    test_rewards_df = pd.DataFrame(test_rewards)\n",
        "    plot_df(test_rewards_df, save_dir + \"testing_rewards.png\", \"Testing Rewards per Episode\", \"Episode\", \"Reward\")\n",
        "    wandb.log({\"Testing Rewards\": wandb.Image(save_dir + \"testing_rewards.png\")})\n",
        "\n",
        "    print(\"Testing Completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
